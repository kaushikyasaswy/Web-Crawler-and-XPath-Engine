Instructions to run:

-> Setup the project as a dynamic web module in eclipse and run on a server instance such as Tomcat

-> Alternatively, you can also do 'ant build' in the project directory to generate a war that can be deployed on a server

Description:

The project consists of two modules. The first one is an Xpath Engine and the second is a Web Crawler. 

XPath Engine:

The XPath engine takes a URL to a document and a set of xpaths as input. It evaluates each of the xpaths against the document and dislays the result. The result for each xpath is either 'Invalid Xpath' or 'Success' or 'Failure'. Once the program is running, the path ~/xpath should present the user with a form asking him to enter the URL to the document and one or more xpaths. 

The grammar that defines the xpaths for this project is as follows
Xpath	->	axis step
axis	->	/
step	->	nodename([test])*(axis step)? 
test	->	step
	->	text() = "..."
	->	contains(text(), "...")
	->	@attname = "..."
where nodename and attname are valid XML identifiers, and "..." indicates a quoted string.

Some examples for valid xpaths are
/foo/bar/xyz   
/foo/bar[@att="123"]   
/xyz/abc[contains(text(),"someSubstring")]   
/a/b/c[text()="theEntireText"]   
/blah[anotherElement]   
/this/that[something/else]   
/d/e/f[foo[text()="something"]][bar]  
/a/b/c[text() =   "whiteSpacesShouldNotMatter"] 

I used the concept of a recursive descent parser to check the validity of the xpaths. For evaluating the xpaths against the document, I used a DOM parser to validate each xpath step by step.

Web Crawler:

A web cralwer crawls for documents or data matching a particular category, just like RSS Feeds and the category will be specified as an XPath expression. A user can create an account after which he can log in and create a new channel. The channel defines the xpaths and in addition a link to a xsl stylesheet. Some design aspects of the web crawler are:
-> I used Berkeley DB for the backend. There are entity stores for storing the cralwed URLs with their content, the users log in information with his channels etc.
-> The number of URLs in the queue may grow exponentially, hence I used an on-disk queue to store all the URLs. An on-disk queue is effectively a database store with a wrapper that hypothetically enqueues at one end and dequeues at the other. 
-> I used a multithreaded design with a thread pool. Hence, a set of threads effectively process the URLs in the queue parallely. This increased the performance of the web crawler by a huge margin.
-> Importance is given to the robots.txt files of each domain that the cralwer tries to crawl. The robots.txt files are stores in an entity store too. This store is flushed for every new iteration of the cralwer. 
-> My own implementation of a HTTP Client to understand how the requests are sent and retrieve the responses.
